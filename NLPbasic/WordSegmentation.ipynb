{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# jieba分词\n",
    "#### 安装：pip install jieba\n",
    "#### 国内源安装更快：pip install jieba -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入jieba包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 全模式分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\yuquanle\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.968 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全模式: 我 爱 自然 自然语言 语言 处理 技术  \n"
     ]
    }
   ],
   "source": [
    "wordseg_all = jieba.cut(\"我爱自然语言处理技术！\", cut_all=True)\n",
    "print(\"全模式: \" + \" \".join(wordseg_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 精确模式分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精确模式: 我 爱 自然语言 处理 技术 ！\n"
     ]
    }
   ],
   "source": [
    "wordseg = jieba.cut(\"我爱自然语言处理技术！\", cut_all=False)\n",
    "print(\"精确模式: \" + \" \".join(wordseg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 搜索引擎模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索引擎模式:我 爱 自然 语言 自然语言 处理 技术 ！\n"
     ]
    }
   ],
   "source": [
    "wordseg_search = jieba.cut_for_search(\"我爱自然语言处理技术！\")  \n",
    "print(\"搜索引擎模式:\" + \" \".join(wordseg_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnowNLP分词\n",
    "#### 安装：pip install snownlp \n",
    "#### 国内源安装：pip install snownlp  -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入snownlp包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '爱', '自然', '语言', '处理', '技术', '！']\n"
     ]
    }
   ],
   "source": [
    "model = SnowNLP(u'我爱自然语言处理技术！')\n",
    "print(model.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THULAC分词\n",
    "#### 安装：pip install thulac    \n",
    "#### 国内源安装：pip install thulac     -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入thulac包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import thulac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 默认模式：分词的同时进行词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n",
      "[['我', 'r'], ['爱', 'v'], ['自然', 'n'], ['语言', 'n'], ['处理', 'v'], ['技术', 'n'], ['！', 'w']]\n"
     ]
    }
   ],
   "source": [
    "thulac_model = thulac.thulac()\n",
    "wordseg = thulac_model.cut(\"我爱自然语言处理技术！\")\n",
    "print(wordseg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 只进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n",
      "[['我', ''], ['爱', ''], ['自然', ''], ['语言', ''], ['处理', ''], ['技术', ''], ['！', '']]\n"
     ]
    }
   ],
   "source": [
    "seg_only_model = thulac.thulac(seg_only=True)\n",
    "wordseg_only = seg_only_model.cut(\"我爱自然语言处理技术！\")\n",
    "print(wordseg_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLPIR分词\n",
    "#### 安装：pip install pynlpir    \n",
    "#### 国内源安装：pip install pynlpir -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入pynlpir包\n",
    "#### 如果发现加载报错，则需要更换license：https://github.com/NLPIR-team/NLPIR/tree/master/License/\n",
    "#### 真是开源的不彻底~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pynlpir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 打开分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pynlpir.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词：这个工具会同时进行词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('我', 'pronoun'), ('爱', 'verb'), ('自然', 'adjective'), ('语言', 'noun'), ('处理', 'verb'), ('技术', 'noun'), ('！', 'punctuation mark')]\n"
     ]
    }
   ],
   "source": [
    "s = \"我爱自然语言处理技术！\"\n",
    "word_seg = pynlpir.segment(s)\n",
    "print(word_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CoreNLP分词\n",
    "#### 安装：pip install stanfordcorenlp    \n",
    "#### 国内源安装：pip install stanfordcorenlp -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入stanfordcorenlp包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 先下载模型，然后导入\n",
    "#### 下载地址："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp_model = StanfordCoreNLP(r'stanford-corenlp-full-2018-02-27', lang='zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我爱', '自然', '语言', '处理', '技术', '！']\n"
     ]
    }
   ],
   "source": [
    "s = '我爱自然语言处理技术！'\n",
    "word_seg = nlp_model.word_tokenize(s)\n",
    "print(word_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hanlp分词\n",
    "#### 安装：pip install pyhanlp     \n",
    "#### 国内源安装：pip install pyhanlp  -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入pyhanlp包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我\n",
      "爱\n",
      "自然语言处理\n",
      "技术\n",
      "！\n"
     ]
    }
   ],
   "source": [
    "s = '我爱自然语言处理技术！'\n",
    "word_seg = HanLP.segment(s)\n",
    "for term in word_seg:\n",
    "    print(term.word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
