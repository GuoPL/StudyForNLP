{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  nlpnet语义角色标注\n",
    "#### 安装：pip install nlpnet   \n",
    "#### 国内源安装：pip install nlpnet -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.nlpnet是一个基于神经网络的自然语言处理任务的Python库。\n",
    "目前，它支持词性标注、依存分析以及语义角色标记。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.首先要下载预训练模型：http://nilc.icmc.usp.br/nlpnet/models.html#srl-portuguese\n",
    "目前语义角色标注只提供了葡萄牙语的预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('roeu',\n",
       "  {'A0': ['O', 'rato'],\n",
       "   'A1': ['a', 'roupa', 'do', 'rei', 'de', 'Roma'],\n",
       "   'V': ['roeu']})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = nlpnet.SRLTagger('nlpnet-model\\srl-pt', language='pt')\n",
    "sents = tagger.tag(u'O rato roeu a roupa do rei de Roma.')[0]\n",
    "sents.arg_structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  pyltp语义角色标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyltp安装有点麻烦-.-，这里记录window 10下的安装方法\n",
    "1.首先，pip install pyltp安装报错：error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\VC\\\\BIN\\\\x86_amd64\\\\cl.exe' failed with exit status 2\n",
    "\n",
    "- 安装cmake，下载地址，https://cmake.org/download/ \n",
    "- 安装VS2008 EXPRESS，下载网址：https://visualstudio.microsoft.com/zh-hans/vs/express/\n",
    "\n",
    "2.然后，我选择使用python setup.py install安装\n",
    "- 下载pyltp，地址：https://github.com/hit-scir/pyltp\n",
    "- 下载ltp，地址：https://github.com/hit-scir/ltp \n",
    "- 解压ltp，然后将解压之后文件命名为ltp，覆盖pyltp文件夹中的ltp\n",
    "- 打开cmd，进入到pyltp目录下，找到setup.py\n",
    "- 先执行命令：python setup.py build\n",
    "- 然后执行命令：python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用里面的预训练模型，需要先下载，然后指定相应目录\n",
    "- 下载地址：http://ltp.ai/download.html\n",
    "\n",
    "#### 要先进行分词，词性标注，依存分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"我爱自然语言处理技术！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 爱 自然 语言 处理 技术 ！\n"
     ]
    }
   ],
   "source": [
    "from pyltp import Segmentor\n",
    "seg = Segmentor() #生成对象\n",
    "seg.load(\"pyltp-model\\ltp_data_v3.4.0\\cws.model\") #加载分词预训练模型\n",
    "seg_words = seg.segment(sentence)\n",
    "print(\" \".join(seg_words))\n",
    "seg.release() #释放资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我\tr\n",
      "爱\tv\n",
      "自然\tn\n",
      "语言\tn\n",
      "处理\tv\n",
      "技术\tn\n",
      "！\twp\n"
     ]
    }
   ],
   "source": [
    "from pyltp import Postagger  \n",
    "pos=Postagger()\n",
    "#加载词性预训练模型\n",
    "pos.load(\"pyltp-model\\ltp_data_v3.4.0\\pos.model\")\n",
    "words_pos=pos.postag(seg_words)\n",
    "for k,v in zip(seg_words, words_pos):\n",
    "    print(k+'\\t'+v)\n",
    "pos.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 'SBV'), (0, 'HED'), (4, 'ATT'), (5, 'FOB'), (2, 'VOB'), (5, 'VOB'), (2, 'WP')]\n"
     ]
    }
   ],
   "source": [
    "from pyltp import Parser\n",
    "parser=Parser()\n",
    "parser.load(\"pyltp-model\\ltp_data_v3.4.0\\parser.model\")\n",
    "arcs=parser.parse(seg_words,words_pos)\n",
    "print([(arc.head,arc.relation) for arc in arcs])\n",
    "parser.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 A0:(0,0)A1:(2,5)\n",
      "4 A1:(5,5)\n"
     ]
    }
   ],
   "source": [
    "from pyltp import SementicRoleLabeller\n",
    "labeller = SementicRoleLabeller()\n",
    "labeller.load(\"pyltp-model\\ltp_data_v3.4.0\\pisrl_win.model\")\n",
    "roles = labeller.label(seg_words,words_pos,arcs)\n",
    "for role in roles:\n",
    "    print(role.index, \"\".join(\n",
    "        [\"%s:(%d,%d)\" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))\n",
    "    \n",
    "labeller.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
